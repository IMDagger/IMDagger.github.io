Разбиение текста на слова
=========================
:date: 2013-02-22 00:59:40
:tags: программирование, текст, слово, python, разбиение, русский, unicode, regex, шинглы
:category: text
:author: IMDagger
:yaru-link: http://imdagger.ya.ru/replies.xml?item_no=1742

Сначала я реализовал черновой вариант с использованием стандартной
библиотеки unicodedata и прохода по символам текста:

.. code-block:: python

            import unicodedata
            ...
            def clean_and_split(self, stop_words):
                words = [u'']
                for letter in self.text.lower():
                    cname = unicodedata.category(letter)[0]
                    if cname in ('Z', 'C', 'P') and letter != '-':
                        if len(words[-1]) > 0:
                            words.append(u'')
                    elif cname == 'L':
                        # don't distinguish letter E and E-with-dots
                        if letter == u'ё':
                            letter = u'е'
                        words[-1] += letter

Но профайлер показывал, что это вторая функция по ресурсам после
базы данных в коде. Поэтому пришлось подумать и переделать это всё в
более короткий алгоритм. Я чувствовал, что всё это стоит записать
каким-нибудь регулярным выражением, т.к. регулярные выражения
преобразуются в конечный автомат, который будет работать быстро.
Оказалось, что стандартная библиотека **re** дружит с Unicode, но
немного не тем боком, что мне нужно. Т.к. я хочу выделить в тексте
только слова из букв (с запасом того, что мне может потребоваться
расширить это условие), выбросив все знаки препинания, цифры и т.д.
Оказалось, что библиотека regex из PyPi как раз то, что нужно:

.. code-block:: python

        import regex
        ...
        # minimal word length is 3 (last letter of word can't be dash)
        SPLITTER = regex.compile(ur'[\p{L}-]{2,}[\p{L}]')

            ...
            def clean_and_split(self, stop_words):
                # тут ещё не хватает проверки stop_words, но добавить её не сложно
                return SPLITTER.findall(self.text.lower().replace(u'ё', u'е'))

Главное не забыть указать префиксы строки **ur** для регулярного
выражения. Т.е. важно не забыть прописать перфикс юникода, иначе \\p{…}
не будет работать как следует. В результате этот участок кода ускорился
от 8 до 10 раз.

Потом мне ещё потребовалось узнать в каких позициях стояли слова, но
это было уже без особых проблем, достаточно использовать finditer метод
объекта скомпилированного регулярного выражения:

.. code-block:: python

        ...
        for match in SPLITTER.finditer(some_text):
            print match.group(), match.start()

Вообще это всё нужно, чтобы разделить текст, выбросить лишние слова
и затем можно уже строить шинглы по нему.
